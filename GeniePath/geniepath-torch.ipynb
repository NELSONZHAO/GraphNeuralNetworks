{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import dgl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GeniePath Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Breadth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GATConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveBreadthLayer(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, num_heads=1):\n",
    "        super(AdaptiveBreadthLayer, self).__init__()\n",
    "        self.gat = GATConv(in_feats=in_dim, \n",
    "                           out_feats=h_dim, \n",
    "                           num_heads=num_heads, \n",
    "                           activation=torch.tanh)\n",
    "        \n",
    "    def forward(self, g, h):\n",
    "        h = self.gat(g, h)\n",
    "        h = h.mean(dim=1)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveDepthLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(AdaptiveDepthLayer, self).__init__()\n",
    "        # input gate\n",
    "        self.input_gate = nn.Linear(in_dim, out_dim)\n",
    "        # forget gate\n",
    "        self.forget_gate = nn.Linear(in_dim, out_dim)\n",
    "        # output gate\n",
    "        self.output_gate = nn.Linear(in_dim, out_dim)\n",
    "        # state C\n",
    "        self.state = nn.Linear(in_dim, out_dim)\n",
    "    \n",
    "    def forward(self, c, h):\n",
    "        # input gate\n",
    "        i = torch.sigmoid(self.input_gate(h))\n",
    "        f = torch.sigmoid(self.forget_gate(h))\n",
    "        o = torch.sigmoid(self.output_gate(h))\n",
    "        c_tilde = torch.tanh(self.state(h))\n",
    "        \n",
    "        c = f * c + i * c_tilde\n",
    "        h = o * torch.tanh(c)\n",
    "        \n",
    "        return c, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeniePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeniePath(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, out_dim, depth, lazy_mode=True):\n",
    "        \"\"\"\n",
    "        @in_dim: 输入维度\n",
    "        @h_dim: 隐藏层的维度\n",
    "        @out_dim: 输出维度\n",
    "        @depth: 迭代深度\n",
    "        \"\"\"\n",
    "        super(GeniePath,  self).__init__()\n",
    "        self.depth = depth\n",
    "        self.lazy_mode = lazy_mode\n",
    "        \n",
    "        self.wx = nn.Linear(in_dim, h_dim)\n",
    "        \n",
    "        self.breath_fn = torch.nn.ModuleList()\n",
    "        self.depth_fn = torch.nn.ModuleList()\n",
    "        for _ in range(depth):\n",
    "            self.breath_fn.append(AdaptiveBreadthLayer(h_dim, h_dim))\n",
    "            if not self.lazy_mode:\n",
    "                self.depth_fn.append(AdaptiveDepthLayer(h_dim, h_dim))\n",
    "            else:\n",
    "                self.depth_fn.append(AdaptiveDepthLayer(2*h_dim, h_dim))\n",
    "\n",
    "        # 输出层\n",
    "        self.out_layer = nn.Linear(h_dim, out_dim)\n",
    "    \n",
    "    def forward(self, g, x):\n",
    "        h0 = self.wx(x)\n",
    "        h = h0\n",
    "        c = torch.zeros_like(h)\n",
    "        \n",
    "        # standard模式\n",
    "        if not self.lazy_mode:\n",
    "            for i in range(self.depth):\n",
    "                h = self.breath_fn[i](g, h)\n",
    "                c, h = self.depth_fn[i](c, h)\n",
    "            \n",
    "            out = torch.relu(self.out_layer(h))\n",
    "            return out\n",
    "        \n",
    "        # lazy模式\n",
    "        else:  \n",
    "            collector = []\n",
    "            for i in range(self.depth):\n",
    "                h = self.breath_fn[i](g, h)\n",
    "                collector.append(h)\n",
    "                \n",
    "            mu = h0\n",
    "            for i in range(self.depth):\n",
    "                h_mu = torch.cat([collector[i], mu], dim=1)\n",
    "                c, mu = self.depth_fn[i](c, h_mu)\n",
    "            \n",
    "            out = torch.relu(self.out_layer(mu))\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application on DataSets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## citation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import citation_graph  as citegrh\n",
    "from dgl import DGLGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citegrh_data = citegrh.load_cora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target\n",
    "features = torch.FloatTensor(citegrh_data.features)\n",
    "labels = torch.LongTensor(citegrh_data.labels)\n",
    "\n",
    "# mask\n",
    "train_mask = torch.BoolTensor(citegrh_data.train_mask)\n",
    "val_mask = torch.BoolTensor(citegrh_data.val_mask)\n",
    "test_mask = torch.BoolTensor(citegrh_data.test_mask)\n",
    "\n",
    "# graph\n",
    "g = DGLGraph(citegrh_data.graph)\n",
    "g = g.add_self_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "in_dim = features.size()[-1]\n",
    "h_dim = 4\n",
    "out_dim = citegrh_data.num_classes\n",
    "depth = 1\n",
    "lazy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "net = GeniePath(in_dim, h_dim, out_dim, depth, lazy)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# main loop\n",
    "dur = []\n",
    "epoch_losses = []\n",
    "best_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(1000):\n",
    "\n",
    "    logits = net(g, features)\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    loss = F.nll_loss(logp[train_mask], labels[train_mask])\n",
    "    epoch_losses.append(loss)\n",
    "\n",
    "    # Compute prediction\n",
    "    pred = logits.argmax(1)\n",
    "\n",
    "    # Compute accuracy on training/validation/test\n",
    "    train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "    val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "    \n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_epoch = epoch\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Train: {:.4f} | Val: {:.4f}\".format(\n",
    "        epoch, loss.item(), train_acc, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pubmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import PubmedGraphDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_data = PubmedGraphDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.FloatTensor(pub_data.features)\n",
    "labels = torch.LongTensor(pub_data.labels)\n",
    "train_mask = torch.BoolTensor(pub_data.train_mask)\n",
    "val_mask = torch.BoolTensor(pub_data.val_mask)\n",
    "test_mask = torch.BoolTensor(pub_data.test_mask)\n",
    "g = DGLGraph(pub_data.graph)\n",
    "g = g.add_self_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "in_dim = features.size()[-1]\n",
    "h_dim = 8\n",
    "out_dim = pub_data.num_classes\n",
    "depth = 3\n",
    "lazy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "net = GeniePath(in_dim, h_dim, out_dim, depth, lazy)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# main loop\n",
    "dur = []\n",
    "epoch_losses = []\n",
    "best_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "\n",
    "    logits = net(g, features)\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    loss = F.nll_loss(logp[train_mask], labels[train_mask])\n",
    "    epoch_losses.append(loss)\n",
    "\n",
    "    # Compute prediction\n",
    "    pred = logits.argmax(1)\n",
    "\n",
    "    # Compute accuracy on training/validation/test\n",
    "    train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "    val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "    \n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_epoch = epoch\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Train: {:.4f} | Val: {:.4f}\".format(\n",
    "        epoch, loss.item(), train_acc, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import PPIDataset\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PPIDataset(mode=\"train\")\n",
    "val_data = PPIDataset(mode=\"valid\")\n",
    "test_data = PPIDataset(mode=\"test\")\n",
    "g = data.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_op = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "in_dim = data.features.shape[-1]\n",
    "h_dim = 8\n",
    "out_dim = data.labels.shape[-1]\n",
    "depth = 3\n",
    "lazy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = GeniePath(in_dim, h_dim, out_dim, depth, lazy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-2, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# main loop\n",
    "dur = []\n",
    "epoch_losses = []\n",
    "# best_f1 = 0\n",
    "# best_epoch = 0\n",
    "for epoch in range(5000):\n",
    "\n",
    "    logits = net(g, torch.FloatTensor(data.features))\n",
    "    torch.nn.BCEWithLogitsLoss()\n",
    "    loss = loss_op(logits, torch.FloatTensor(data.labels))\n",
    "    epoch_losses.append(loss)\n",
    "\n",
    "    # Compute prediction\n",
    "    pred_val = net(val_data.graph, torch.FloatTensor(val_data.features))\n",
    "    pred_val = (pred_val.detach().numpy()>0).astype(int)\n",
    "\n",
    "    micro_f1_val = f1_score(val_data.labels, pred_val, average='micro')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Val: {:.4f} | Test: {:.4f}\".format(\n",
    "        epoch, loss.item(), micro_f1_val, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
